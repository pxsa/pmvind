{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Segment Type</th>\n",
       "      <th>Segment Description</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Global results</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>548</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Global results</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>916</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Global results</td>\n",
       "      <td>Snapchat</td>\n",
       "      <td>86</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Global results</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>179</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Global results</td>\n",
       "      <td>None</td>\n",
       "      <td>947</td>\n",
       "      <td>0.354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question Segment Type  \\\n",
       "0  What social platform has influenced your onlin...       Mobile   \n",
       "1  What social platform has influenced your onlin...       Mobile   \n",
       "2  What social platform has influenced your onlin...       Mobile   \n",
       "3  What social platform has influenced your onlin...       Mobile   \n",
       "4  What social platform has influenced your onlin...       Mobile   \n",
       "\n",
       "  Segment Description     Answer  Count  Percentage  \n",
       "0      Global results   Facebook    548       0.205  \n",
       "1      Global results  Instagram    916       0.342  \n",
       "2      Global results   Snapchat     86       0.032  \n",
       "3      Global results    Twitter    179       0.067  \n",
       "4      Global results       None    947       0.354  "
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/WhatsgoodlyData-6.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop numerical columns\n",
    "df.drop(columns=['Count','Percentage'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question               0\n",
       "Segment Type           0\n",
       "Segment Description    0\n",
       "Answer                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of missing values per column.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Question', 'Segment Type', 'Segment Description', 'Answer'], dtype='object')"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Segment Type</th>\n",
       "      <th>Segment Description</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Global results</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Global results</td>\n",
       "      <td>Instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Global results</td>\n",
       "      <td>Snapchat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Global results</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Global results</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Web</td>\n",
       "      <td>Web</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Web</td>\n",
       "      <td>Web</td>\n",
       "      <td>Instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Web</td>\n",
       "      <td>Web</td>\n",
       "      <td>Snapchat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Web</td>\n",
       "      <td>Web</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What social platform has influenced your onlin...</td>\n",
       "      <td>Web</td>\n",
       "      <td>Web</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question Segment Type  \\\n",
       "0  What social platform has influenced your onlin...       Mobile   \n",
       "1  What social platform has influenced your onlin...          NaN   \n",
       "2  What social platform has influenced your onlin...       Mobile   \n",
       "3  What social platform has influenced your onlin...       Mobile   \n",
       "4  What social platform has influenced your onlin...       Mobile   \n",
       "5  What social platform has influenced your onlin...          Web   \n",
       "6  What social platform has influenced your onlin...          Web   \n",
       "7  What social platform has influenced your onlin...          Web   \n",
       "8  What social platform has influenced your onlin...          Web   \n",
       "9  What social platform has influenced your onlin...          Web   \n",
       "\n",
       "  Segment Description     Answer  \n",
       "0      Global results   Facebook  \n",
       "1      Global results  Instagram  \n",
       "2      Global results   Snapchat  \n",
       "3      Global results    Twitter  \n",
       "4      Global results       None  \n",
       "5                 Web   Facebook  \n",
       "6                 Web  Instagram  \n",
       "7                 Web   Snapchat  \n",
       "8                 Web        NaN  \n",
       "9                 Web       None  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nullify some fields at random\n",
    "df = df.stack().sample(frac=0.9).unstack()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question               127\n",
       "Segment Type           170\n",
       "Segment Description    139\n",
       "Answer                 144\n",
       "dtype: int64"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractCompleteTuples(df):\n",
    "    # getting the rows without null values\n",
    "    CT = df.dropna()\n",
    "    return CT   # CT.shape #(959, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractInCompleteTuples(df):\n",
    "    # getting only the rows with null values\n",
    "    ICT = df[df.isnull().any(axis=1)]\n",
    "    # print(ICT.shape) #(41, 5)\n",
    "    return ICT.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log,e\n",
    "\n",
    "# Entropy weight method (EWM)\n",
    "def ComputeAttributeWeights(CT):\n",
    "    n = CT.shape[0] # the number of rows in complete tuples\n",
    "    s = CT.shape[1] # the number of columns\n",
    "\n",
    "    # 1- Normalizing data(just numerocal cols)\n",
    "\n",
    "    # 2-1 Calculating the entropy of each numerical attribute   \n",
    "    \n",
    "    # 2-2 Calculating the entropy of each categorical attribute \n",
    "    def entropy(labels, base=None):\n",
    "        vc = pd.Series(labels).value_counts(normalize=True, sort=False)\n",
    "        base = e if base is None else base\n",
    "        return -(vc * np.log(vc)/np.log(base)).sum()\n",
    "\n",
    "    E = []         \n",
    "    for column in CT:\n",
    "        E.append(entropy(CT[column], base=None))\n",
    "        \n",
    "    # 3- Determining the weight of each attribute\n",
    "    w = [0] * s     \n",
    "    k = s\n",
    "    sum = 0\n",
    "    for i in range(k):\n",
    "        sum += E[i]\n",
    "\n",
    "    for i in range(s):\n",
    "        w[i] = (1 - E[i]) / (k - sum)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SortInCompleteTuples(ICT, r):\n",
    "    # Convert list to npArray\n",
    "    r = np.array(r)\n",
    "\n",
    "    # Arg sort\n",
    "    argSort = np.argsort(r) # it sorts r, and returns corresponding indexes\n",
    "    \n",
    "    # Create new empty npArray for sorted ICT\n",
    "    sortedICT = np.copy(ICT)\n",
    "    for index in range(argSort.size):\n",
    "        sortedICT[index] = ICT[argSort[index]]\n",
    "    \n",
    "    return sortedICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Partition(seq, num):\n",
    "    avg  = len(seq) / float(num)\n",
    "    out  = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateTuplePartition(sortedICT, m):\n",
    "    T = []\n",
    "    T = Partition(sortedICT, m)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateTuplePartitions(ICT, CT, m, s):\n",
    "    W = ComputeAttributeWeights(CT)\n",
    "    # STEP 1\n",
    "    # Calculate tuple integrity rate, according to DEFINITION 5(example)\n",
    "    inCompleteRowsCount = ICT.shape[0]      # the number of ICT rows                     \n",
    "    for i in range(inCompleteRowsCount):\n",
    "        for j in range(s):\n",
    "            if pd.isnull(ICT[i][j]):\n",
    "                r[i] = r[i] - W[j]         \n",
    "\n",
    "    # STEP 2\n",
    "    # sort ICT's tuples according to their integrity rate\n",
    "    sortedICT = SortInCompleteTuples(ICT, r)\n",
    "    \n",
    "    # STEP 3\n",
    "    tuplePartitions = GenerateTuplePartition(sortedICT, m)\n",
    "    return tuplePartitions # a queue of subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function which calculates euclidean distance between two data points(numerical)\n",
    "def euclideanDistance(data1, data2, length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += np.square(data1[x] - data2[x])\n",
    "    return np.sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(complete_set,incomplete_set, numeric_distance = \"euclidean\", categorical_distance = \"hamming\"):\n",
    "    \n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in complete_set.iloc[:, i]) for i, x in enumerate(complete_set)]\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "\n",
    "    if categorical_distance == 'hamming':\n",
    "        complete_set = pd.DataFrame([pd.factorize(complete_set[x])[0] for x in complete_set]).transpose()\n",
    "        incomplete_set = pd.DataFrame([pd.factorize(incomplete_set[x])[0] for x in incomplete_set]).transpose()\n",
    "\n",
    "    if is_all_categorical:\n",
    "        if categorical_distance == \"hamming\":\n",
    "            result_matrix = cdist(complete_set, incomplete_set, metric=categorical_distance)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_impute(complete_set, incomplete_set, k_neighbors, aggregation_method=\"mode\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"hamming\"):\n",
    "    \n",
    "    numberOfICSamples=len(incomplete_set)\n",
    "    target=[]\n",
    "    inc_set=incomplete_set.copy()\n",
    "    \n",
    "    # Make sure the data are in the right format\n",
    "    inc_set = pd.DataFrame(inc_set)\n",
    "    complete_set = pd.DataFrame(complete_set)\n",
    "    \n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(complete_set,inc_set, numeric_distance, categorical_distance)\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for j in range(numberOfICSamples):\n",
    "        for i, value in enumerate(inc_set.iloc[j, :]):\n",
    "            if pd.isnull(value):\n",
    "                order = distances.iloc[:,i].values.argsort()[:k_neighbors]\n",
    "                closest_to_target = complete_set.iloc[order, i]\n",
    "                inc_set.iloc[j,i]=stats.mode(closest_to_target)[0][0]\n",
    "    \n",
    "    return inc_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNImputation(train_set, test_set):\n",
    "    return knn_impute(train_set, test_set, k_neighbors=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge(a, b):\n",
    "    a=np.array(a)\n",
    "    b=np.array(b)\n",
    "    return np.concatenate((a, b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mean(a, b):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin\n",
    "CT  = ExtractCompleteTuples(df)   # this is dataframe\n",
    "ICT = ExtractInCompleteTuples(df) # this is npArray\n",
    "\n",
    "# The number of partitions\n",
    "m = 100 # TODO ?\n",
    "# The number of attributes\n",
    "s = df.columns.size\n",
    "\n",
    "r = [1] * (ICT.shape[0])\n",
    "T = GenerateTuplePartitions(ICT, CT, m, s)\n",
    "\n",
    "CTS    = [[0]] * (m+1)\n",
    "Tp     = [[0]] * (m+1)\n",
    "CTS[0] = np.array(CT.copy())\n",
    "\n",
    "for i in range(1, m+1):\n",
    "    Tp[i-1] = KNNImputation(CTS[i-1], T[i-1])\n",
    "    CTS[i]  = Merge(CTS[i-1], Tp[i-1])\n",
    "\n",
    "\n",
    "# Do cross validation\n",
    "Tpp = [[0]] * (m+1)\n",
    "for i in range(0, m):\n",
    "    Tpp[i] = KNNImputation(train_set=CTS[m], test_set=T[i])\n",
    "\n",
    "Tpp[m] = KNNImputation(train_set=CTS[0], test_set=T[m-1])    \n",
    "\n",
    "for i in range(1, m+1):\n",
    "    CTS[i] = Merge(CTS[i-1],Tpp[i-1])\n",
    "\n",
    "# print(CTS[m].shape) #(1450, 4)\n",
    "\n",
    "# D' = CT[m] is our complete dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Evaluation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1450, 4), indices imply (1450, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [317]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# convert CTS[m] to a dataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCTS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgender\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrace/ethnicity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparental level of education\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlunch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest preparation course\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# df = df.dropna()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:694\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    684\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    685\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    686\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    692\u001b[0m         )\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:351\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# _prep_ndarray ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    347\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    348\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    349\u001b[0m )\n\u001b[1;32m--> 351\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:422\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    420\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    421\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 422\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (1450, 4), indices imply (1450, 5)"
     ]
    }
   ],
   "source": [
    "# convert CTS[m] to a dataFrame\n",
    "df = pd.DataFrame(CTS[m], columns=['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'])\n",
    "# df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of missing values per column.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender']                      = df['gender'].map({'male':0, 'female':1})\n",
    "df['race/ethnicity']              = df['race/ethnicity'].map({'group A':0, 'group B':1, 'group C':2, 'group D':3, 'group E':4})\n",
    "df['parental level of education'] = df['parental level of education'].map({\"bachelor's degree\":0, \"high school\":1, \"some college\":2, \"master's degree\":3, \"associate's degree\":4, \"some high school\":5})\n",
    "df['lunch']                       = df['lunch'].map({'standard':0, 'free/reduced':1})\n",
    "df['test preparation course']     = df['test preparation course'].map({'none':0, 'completed':1})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(df['gender'])\n",
    "data   = np.array(df.loc[:, df.columns != 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(data, target, random_state=42)\n",
    "\n",
    "\n",
    "print(\"shape of train data:   \", xTrain.shape)\n",
    "print(\"shape of test  data:   \", xTest.shape)\n",
    "print(\"shape of train target: \", yTest.shape)\n",
    "print(\"shape of test  target: \", yTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(df['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(xTrain, yTrain)\n",
    "\n",
    "yPred = knn.predict(xTest)\n",
    "print(knn.score(xTrain, yTrain))\n",
    "print(knn.score(xTest,  yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(yTest, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(yTest, yPred)\n",
    "sns.heatmap(cm, square=True, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PFC metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PFC(NTrue, NFalse):\n",
    "    return (NFalse/(NTrue+NFalse))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractOriginalSet(df_with_NaN,df_without_NaN):\n",
    "     return np.array(df_without_NaN[df_with_NaN.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractPredictSet():\n",
    "    return CTS[m][len(CT):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of correct and incorrect predictions\n",
    "def Calculate_NT_and_NF_of_prediction(originalSet,predictSet,NaNSet):\n",
    "    NTrue=0\n",
    "    NFalse=0\n",
    "    rowsCount=len(NaNSet)\n",
    "    columnsCount=len(NaNSet[0])\n",
    "    \n",
    "    for i in range(rowsCount):\n",
    "        for j in range(columnsCount):\n",
    "            if pd.isnull(NaNSet[i][j]):\n",
    "                if(originalSet[i][j]==predictSet[i][j]):\n",
    "                    NTrue+=1\n",
    "                else:\n",
    "                    NFalse+=1\n",
    "    return {\n",
    "        \"NTrue\":NTrue,\n",
    "        \"NFalse\":NFalse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SortOriginalSet(originalSet):\n",
    "    sortedOriginalSet = SortInCompleteTuples(originalSet, r)   \n",
    "    return sortedOriginalSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SortNaNSet():\n",
    "    sortedNanSet=[0]* (len(ICT))\n",
    "    index=0\n",
    "    for i in range(len(T)):\n",
    "        for j in range(len(T[i])):\n",
    "            sortedNanSet[index]=T[i][j]\n",
    "            index+=1\n",
    "    return sortedNanSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5827586206896552\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"data/WhatsgoodlyData-6.csv\")\n",
    "df2.drop(columns=['Count','Percentage'], inplace=True)\n",
    "\n",
    "originalSet=ExtractOriginalSet(df,df2)\n",
    "originalSet=SortOriginalSet(originalSet)\n",
    "\n",
    "predictSet=ExtractPredictSet()\n",
    "\n",
    "NaNSet=SortNaNSet()\n",
    "\n",
    "result=Calculate_NT_and_NF_of_prediction(originalSet,predictSet,NaNSet)\n",
    "NTrue=result.get(\"NTrue\")\n",
    "NFalse=result.get(\"NFalse\")\n",
    "\n",
    "print(PFC(NTrue,NFalse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b25c3272fda9386675e7f647d3dda333e487e678f3dc1b6bfbf79265cfda035"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
