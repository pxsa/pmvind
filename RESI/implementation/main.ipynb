{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/Students_Performance_knn.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#   Students_Performance_knn.csv\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#   Students_Performance_mv.csv\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#   Wine_Quality.csv\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/Students_Performance_knn.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/Students_Performance_mv.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m df3 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/Wine_Quality.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/Students_Performance_knn.csv'"
     ]
    }
   ],
   "source": [
    "#   Students_Performance_knn.csv\n",
    "#   Students_Performance_mv.csv\n",
    "#   Wine_Quality.csv\n",
    "df1 = pd.read_csv('data/Students_Performance_knn.csv')\n",
    "df2 = pd.read_csv('data/Students_Performance_mv.csv')\n",
    "df3 = pd.read_csv('data/Wine_Quality.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Drop numerical columns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmath score\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreading score\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwriting score\u001b[39m\u001b[38;5;124m'\u001b[39m],inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "# Drop numerical columns\n",
    "df2.drop(columns=['math score','reading score','writing score'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                          0\n",
       "race/ethnicity                 11\n",
       "parental level of education    21\n",
       "lunch                          12\n",
       "test preparation course         4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of missing values per column.\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'race/ethnicity', 'parental level of education', 'lunch',\n",
       "       'test preparation course'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of incomplete subsets\n",
    "m = 5\n",
    "# The number of attributes\n",
    "s = df2.columns.size    # 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'group C'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ExtractCompleteTuples(df):\n",
    "    # getting the rows without null values\n",
    "    CT = df.dropna()\n",
    "    # print(CT.shape) #(959, 8)\n",
    "    # print(CT.min())\n",
    "    CTTransposed = CT.T\n",
    "    return CT\n",
    "    \n",
    "x=ExtractCompleteTuples(df2)\n",
    "y=np.array(x)\n",
    "y=y.transpose()\n",
    "y[1][835]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['female', nan, 'some college', 'free/reduced', 'completed'],\n",
       "       ['female', nan, \"associate's degree\", 'free/reduced', 'none'],\n",
       "       ['male', 'group D', nan, 'standard', 'completed'],\n",
       "       ['male', 'group A', nan, nan, 'completed'],\n",
       "       ['female', nan, nan, 'standard', 'none'],\n",
       "       ['female', 'group E', 'some college', nan, 'completed'],\n",
       "       ['female', nan, \"associate's degree\", 'free/reduced', 'none'],\n",
       "       ['male', 'group C', \"bachelor's degree\", nan, 'none'],\n",
       "       ['male', 'group A', 'high school', nan, 'completed'],\n",
       "       ['female', 'group B', \"bachelor's degree\", nan, 'none'],\n",
       "       ['male', 'group C', 'high school', nan, 'none'],\n",
       "       ['male', 'group E', \"associate's degree\", nan, 'completed'],\n",
       "       ['female', 'group C', 'some high school', nan, 'none'],\n",
       "       ['male', 'group E', nan, nan, 'completed'],\n",
       "       ['male', 'group B', nan, 'standard', 'completed'],\n",
       "       ['female', 'group C', nan, 'free/reduced', 'completed'],\n",
       "       ['male', 'group B', nan, 'free/reduced', 'none'],\n",
       "       ['female', 'group C', nan, 'standard', 'none'],\n",
       "       ['female', 'group D', nan, 'free/reduced', 'none'],\n",
       "       ['male', 'group A', nan, 'free/reduced', 'completed'],\n",
       "       ['male', 'group D', nan, 'standard', 'none'],\n",
       "       ['male', 'group C', nan, 'standard', 'completed'],\n",
       "       ['male', 'group A', nan, 'standard', 'completed'],\n",
       "       ['female', 'group D', nan, 'standard', 'none'],\n",
       "       ['male', 'group D', nan, 'standard', 'none'],\n",
       "       ['female', nan, nan, 'standard', 'completed'],\n",
       "       ['female', nan, 'some college', 'standard', 'none'],\n",
       "       ['male', 'group C', nan, 'standard', 'none'],\n",
       "       ['female', nan, 'some college', 'free/reduced', 'none'],\n",
       "       ['female', 'group E', nan, 'standard', 'none'],\n",
       "       ['male', 'group D', nan, 'standard', 'none'],\n",
       "       ['female', 'group B', nan, 'free/reduced', 'completed'],\n",
       "       ['female', nan, \"associate's degree\", 'standard', 'none'],\n",
       "       ['male', nan, 'some high school', 'standard', 'completed'],\n",
       "       ['male', nan, nan, 'free/reduced', 'none'],\n",
       "       ['male', 'group D', \"bachelor's degree\", nan, 'completed'],\n",
       "       ['female', 'group E', \"associate's degree\", nan, 'none'],\n",
       "       ['male', 'group E', \"bachelor's degree\", 'standard', nan],\n",
       "       ['male', 'group C', 'high school', 'standard', nan],\n",
       "       ['female', 'group D', 'some college', 'free/reduced', nan],\n",
       "       ['female', nan, 'some high school', nan, nan]], dtype=object)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ExtractInCompleteTuples(df):\n",
    "    # getting only the rows with null values\n",
    "    ICT = df[df.isnull().any(axis=1)]\n",
    "    # print(ICT.shape) #(41, 8)\n",
    "    return ICT.values\n",
    "\n",
    "ExtractInCompleteTuples(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min(val):\n",
    "    m = val[0]\n",
    "    for item in val:\n",
    "        if item < m:\n",
    "            m = item\n",
    "    return m\n",
    "\n",
    "def max(val):\n",
    "    m = val[0]\n",
    "    for item in val:\n",
    "        if item > m:\n",
    "            m = item\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "sna = [0]*5\n",
    "print(sna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log,e\n",
    "\n",
    "# Entropy weight method (EWM)\n",
    "def ComputeAttributeWeights(CT):\n",
    "    # CTTransposed = CT.T (assume that CT is already transposed)\n",
    "    n = CT.shape[0]\n",
    "    s = CT.shape[1]\n",
    "\n",
    "    # 1- Normalizing data(just numerocal cols)\n",
    "#     y = [s][n]\n",
    "#     sna = [0] * s # will create a list of size s full of zeros\n",
    "#     for i in range(s):\n",
    "#         for j in range(n):\n",
    "#             y[i][j] = (CT[i][j] - min(CT[i])) / (max(CT[i] - min(CT[i])))\n",
    "#             sna[i] += y[i][j]\n",
    "\n",
    "\n",
    "    # 2-1 Calculating the entropy of each numerical attribute   \n",
    "#     p = [s][n]\n",
    "#     container = [0] * s\n",
    "    \n",
    "#     for i in range(s):\n",
    "#         for j in range(n):\n",
    "#             p[i][j] = y[i][j] / sna[i]\n",
    "#             container[i] += p[i][j] * log(p[i][j])\n",
    "\n",
    "#     E = [0] * s\n",
    "#     print(E)\n",
    "#     for i in range(s):\n",
    "#         for j in range(n):\n",
    "            \n",
    "#             E[i] = -log(container[i] / n)\n",
    "    \n",
    "    \n",
    "    # 2-2 Calculating the entropy of each categorical attribute \n",
    "    def entropy(labels, base=None):\n",
    "        vc = pd.Series(labels).value_counts(normalize=True, sort=False)\n",
    "        base = e if base is None else base\n",
    "        return -(vc * np.log(vc)/np.log(base)).sum()\n",
    "\n",
    "    E=[]\n",
    "    for column in CT:\n",
    "        E.append(entropy(CT[column], base=None))\n",
    "    \n",
    "    # 3- Determining the weight of each attribute\n",
    "    w = [0] * s\n",
    "    k = s\n",
    "    sum = 0\n",
    "    for i in range(k):\n",
    "        sum += E[i]\n",
    "    for i in range(s):\n",
    "        w[i] = (1 - E[i]) / (k - sum)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SortInCompleteTuples(ICT, r):\n",
    "    # Convert list to npArray\n",
    "    r = np.array(r)\n",
    "\n",
    "    # Arg sort\n",
    "    argSort = np.argsort(r) # it sorts r, and returns corresponding indexes\n",
    "    \n",
    "    # Create new empty npArray for sorted ICT\n",
    "    sortedICT = np.copy(ICT)\n",
    "    for index in range(argSort.size):\n",
    "        sortedICT[index] = ICT[argSort[index]]\n",
    "    \n",
    "    return sortedICT\n",
    "\n",
    "# SortInCompleteTuples(ICT, [4, 8, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Partition(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateTuplePartition(sortedICT, m):\n",
    "    T=[]\n",
    "    T=Partition(sortedICT, m)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateTuplePartitions(ICT, CT, m, s):\n",
    "\n",
    "    W = ComputeAttributeWeights(CT)\n",
    "    \n",
    "    # STEP 1\n",
    "    # Calculate tuple intergrity rate, according to DEFINITION 5(example)\n",
    "    inCompleteRowsCount = ICT.shape[0]      # gives us the number of ICT rows\n",
    "    r = [1] * inCompleteRowsCount                      \n",
    "    for i in range(inCompleteRowsCount):\n",
    "        for j in range(s):\n",
    "            if pd.isnull(ICT[i][j]):               # if ICT[i][j] item, not exists then go through\n",
    "                r[i] = r[i] - W[j]    \n",
    "                \n",
    "    # at this part we're sure that we have intergrity rate's list\n",
    "\n",
    "    # STEP 2\n",
    "    # sort ICT's tuples according to their intergrity rate\n",
    "    sortedICT = SortInCompleteTuples(ICT, r)\n",
    "    \n",
    "    # STEP 3\n",
    "    tuplePartitions = GenerateTuplePartition(sortedICT, m)\n",
    "    return tuplePartitions # a queue of subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function which calculates euclidean distance between two data points(numerical)\n",
    "def euclideanDistance(data1, data2, length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += np.square(data1[x] - data2[x])\n",
    "    return np.sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_hamming(data):\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "        \n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "\n",
    "    # Replace missing values with mode for categorical columns\n",
    "    for x in data:\n",
    "        data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        data = pd.get_dummies(data)\n",
    "    elif categorical_distance == 'hamming':\n",
    "        data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "    return pd.DataFrame(result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mode\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "    numberOfSamples=len(target[0])\n",
    "   \n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "    \n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes.transpose(), numeric_distance, categorical_distance)\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for j in range(numberOfSamples):\n",
    "        for i, value in enumerate(target.iloc[:, j]):\n",
    "            if pd.isnull(value):\n",
    "                order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "                closest_to_target = attributes.iloc[i, order]\n",
    "                target.iloc[i,j]=stats.mode(closest_to_target)[0][0]\n",
    "    \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNImputation(train_set, test_set):\n",
    "    knn_impute(test_set.transpose(),train_set.transpose(), k_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge(a, b):\n",
    "    a=np.array(a)\n",
    "    b=np.array(b)\n",
    "    return np.concatenate((a, b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([['female', 'group B', \"bachelor's degree\", 'standard', 'none'],\n",
      "       ['female', 'group C', 'some college', 'standard', 'completed'],\n",
      "       ['female', 'group B', \"master's degree\", 'standard', 'none'],\n",
      "       ...,\n",
      "       ['female', 'group C', 'high school', 'free/reduced', 'completed'],\n",
      "       ['female', 'group D', 'some college', 'standard', 'completed'],\n",
      "       ['female', 'group D', 'some college', 'free/reduced', 'none']],\n",
      "      dtype=object), array([['female', 'group B', \"bachelor's degree\", 'standard', 'none'],\n",
      "       ['female', 'group C', 'some college', 'standard', 'completed'],\n",
      "       ['female', 'group B', \"master's degree\", 'standard', 'none'],\n",
      "       ...,\n",
      "       ['male', 'group D', \"master's degree\", 'standard', 'completed'],\n",
      "       ['male', 'group C', \"master's degree\", 'standard', 'completed'],\n",
      "       ['female', 'group C', 'some college', 'free/reduced', 'none']],\n",
      "      dtype=object), array([['female', 'group B', \"bachelor's degree\", 'standard', 'none'],\n",
      "       ['female', 'group C', 'some college', 'standard', 'completed'],\n",
      "       ['female', 'group B', \"master's degree\", 'standard', 'none'],\n",
      "       ...,\n",
      "       ['male', 'group A', 'high school', 'free/reduced', 'completed'],\n",
      "       ['female', 'group C', 'some high school', 'free/reduced', 'none'],\n",
      "       ['female', 'group D', 'some college', 'free/reduced', 'none']],\n",
      "      dtype=object), array([['female', 'group B', \"bachelor's degree\", 'standard', 'none'],\n",
      "       ['female', 'group C', 'some college', 'standard', 'completed'],\n",
      "       ['female', 'group B', \"master's degree\", 'standard', 'none'],\n",
      "       ...,\n",
      "       ['female', 'group E', \"associate's degree\", 'free/reduced',\n",
      "        'none'],\n",
      "       ['female', 'group B', \"bachelor's degree\", 'free/reduced', 'none'],\n",
      "       ['female', 'group E', 'some college', 'free/reduced', 'completed']],\n",
      "      dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# Begin\n",
    "CT= ExtractCompleteTuples(df2)   # this is dataframe\n",
    "ICT = ExtractInCompleteTuples(df2) # this is npArray\n",
    "\n",
    "m=4 # test \n",
    "T   = GenerateTuplePartitions(ICT, CT, m, s)\n",
    "\n",
    "CTS=[[0]] * (m)\n",
    "CTS[0]=np.array(CT.copy())\n",
    "\n",
    "for i in range(1, m):\n",
    "    KNNImputation(CTS[i-1], T[i])\n",
    "    CTS[i] =  Merge(CTS[i-1], T[i])\n",
    "    \n",
    "print(CTS)     \n",
    "# Do cross validation\n",
    "# for i in range(1, m-1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b25c3272fda9386675e7f647d3dda333e487e678f3dc1b6bfbf79265cfda035"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
